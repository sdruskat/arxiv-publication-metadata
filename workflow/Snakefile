# SPDX-FileCopyrightText: 2024 German Aerospace Center (DLR)
# SPDX-FileContributor: Stephan Druskat <stephan.druskat@dlr.de>
#
# SPDX-License-Identifier: CC0-1.0

import os
import json
from dotenv import load_dotenv
# Main entrypoint of the workflow.
# This file attempts to follow the good practice described at
# https://snakemake.readthedocs.io/en/stable/snakefiles/best_practices.html.

conda:
    "envs/global.yaml"  # Makes storage plugins usable workflow-wide

load_dotenv()
ZENODO_TOKEN = os.environ.get("ZENODO_TOKEN")


storage:
    provider="zenodo",
    access_token=ZENODO_TOKEN,
    restricted_access_token=ZENODO_TOKEN


checkpoint download_metadata:
    """
    Downloads the metadata from Zenodo and extracts them to the target folder.
    """
    input:
        storage.zenodo("zenodo://record/11065282/metha-output-OAI-PMH-arXivRaw-until-2024-03-24.tar.gz")
    output:
        directory("resources/metha-output/")
    shell:
        "mkdir -p {output} && tar -xf {input[0]} --directory {output}"

rule gunzip:
    """
    Takes a set of gzipped XML files with ArXiv publication metadata 
    produced by metha (https://github.com/miku/metha)
    and extracts them to a directory, keeping the original files.
    
    It is assumed that the input files have been produced using this or an analog method:
    
    ```
    # Bash script used in a SLURM definition file
    ## Add Go 1.18 to the runtime environment
    module add go/go-1.18/go-1.18-gcc-9.4.0-okbjyoy
    
    ## Install the latest version of metha
    go install -v github.com/miku/metha/cmd/...@latest
    
    ## Harvest ArXiv OAI-PMH metadata in the format arXivRaw from http://export.arxiv.org/oai2
    metha-sync -T 5m -base-dir metha/harvest/arxiv -format "arXivRaw" http://export.arxiv.org/oai2
    ```
    """
    input:
        'resources/metha-output/{file_name}.xml.gz'
    output:
        "resources/extracted/{file_name}.xml"
    run:
        shell("mkdir -p resources/extracted && gunzip -c {input} > {output}")

rule write_lut:
    """
    Takes XML files with ArXiv OAI-PMH metadata in XML format arXivRaw,
    and for each, writes a JSON file containing a lookup table from
    a versioned ArXiv identifier to a publication date in the format
    `YYYY-MM-DD`. 
    """
    input:
        xml="resources/extracted/{file_name}.xml",
        arxiv_xsd=storage.http("http://arxiv.org/OAI/arXivRaw.xsd"),
    output:
        lut="resources/luts/{file_name}.json"
    script:
        "scripts/write_lut.py"

def _expand_luts(wildcards) -> list[str]:
    metadata_dir = checkpoints.download_metadata.get(**wildcards).output[0]
    file_names = glob_wildcards(os.path.join(metadata_dir,'{file_name}.xml.gz')).file_name
    return expand(
        'resources/luts/{file_name}.json',file_name=file_names
    )



rule merge_luts:
    """
    Takes JSON files containing a dictionary and merges them into a single JSON file.
    """
    input:
        inputs=_expand_luts
    output:
        "resources/arxiv-publication-dates.json"
    script:
        "scripts/merge_luts.py"

checkpoint clone_git:
    """
    Clone the GitHub repository containing the source dataset 'Extract-URLs' locally.
    """
    output:
        git=directory("resources/extract-urls-git")
    shell:
        "git clone --depth 1 -b production --single-branch https://github.com/sdruskat/Extract-URLs.git {output}"


def _expand_parse_arxiv_url_files(wildcards):
    """
    Expands all files names for all JSON files in the directories
    {RESOURCES_DIR}pmc/<given resources_dir>/, and
    {RESOURCES_DIR}arxiv/<given resources_dir>/
    """
    git_dir = checkpoints.clone_git.get(**wildcards).output['git']
    file_names_arxiv = glob_wildcards(os.path.join(git_dir,'parsed/{file_name}.json')).file_name

    return expand("resources/extract-urls-git/parsed/{file_name}.json",file_name=file_names_arxiv)



rule patch_versions:
    """
    Checks whether all ArXiv IDs recorded in the parsed ArXiv part of the Extract-URLs dataset
    and tries to retrieve manually missing versions.
    """
    input:
        lut=rules.merge_luts.output[0],
        arxiv_urls=_expand_parse_arxiv_url_files,
        arxiv_xsd=storage.http("http://arxiv.org/OAI/arXivRaw.xsd"),
    output:
        "resources/arxiv-publication-dates-patched.json"
    threads: 1
    script:
        "scripts/patch_versions.py"

checkpoint split_luts:
    """
    Splits existing JSON LUTs mapping ArXiv version identifiers
    to publication dates by ArXiv identifier prefix (YYMM),
    and saves them into separate files. 
    """
    input:
        "resources/arxiv-publication-dates-patched.json"
    output:
        directory("resources/split-luts")
    script:
        "scripts/split_luts.py"

def _expand_split_luts(wildcards):
    split_luts_dir = checkpoints.split_luts.get(**wildcards).output[0]
    file_names_luts = glob_wildcards(os.path.join(split_luts_dir,'{file_name}.json')).file_name
    return expand(os.path.join(split_luts_dir, '{file_name}.json'),file_name=file_names_luts)

rule all:
    """
    Produces an archive file containing lookup JSON files (<YYMM>.json) mapping
    versioned ArXiv publication identifiers to publication dates in
    the format 'YYYY-MM-DD'. 
    """
    default_target: True
    input:
        _expand_split_luts
    output:
        tgz="results/arxiv-publication-dates-by-identifier-prefix.tar.gz",
        name_json="results/file_names.json",
    run:
        with open(output.name_json, "w") as name_f:
            json.dump([fn.split("/")[-1] for fn in input], name_f)
        files = " ".join([fn.split("/")[-1] for fn in [str(fn) for fn in {input}][0].split(" ")])
        shell("tar -czvf {output.tgz} -C {rules.split_luts.output[0]} {files}")

rule clean:
    threads: 1
    shell:
        "rm -rf resources/ results/ .cache/ .conda/ .snakemake/ logs/ results.log"  # Keeping extracted files
